[{
  "name": "Nidum-Llama 3.2 3B Uncensored",
  "size": "2.24 GB",
  "id": "nidum_llama3.2_3b_uncensored",
  "file_name": "model-Q4_K_M.gguf",
  "hf_link": "hf:nidum/Nidum-Llama-3.2-3B-Uncensored-GGUF/model-Q4_K_M.gguf",
  "category": "Specialized - Medium",
  "description": "Nidum-Llama 3.2 3B Uncensored is an instruction-tuned model offering unrestricted responses, utilizing 4-bit grouped quantization (Q4_K_M). (Requires 8GB RAM)",
  "info_url": "https://huggingface.co/nidum/Nidum-Llama-3.2-3B-Uncensored-GGUF"
},{
  "name": "Nidum-Llama 3.2 3B Uncensored (Q8_0)",
  "size": "4.50 GB",
  "id": "nidum_llama3.2_3b_uncensored_q8",
  "file_name": "nidum-llama-3.2-3b-uncensored-q8_0.gguf",
  "hf_link": "hf:Melvin56/Nidum-Llama-3.2-3B-Uncensored-Q8_0-GGUF/nidum-llama-3.2-3b-uncensored-q8_0.gguf",
  "category": "Specialized - Medium",
  "description": "Nidum-Llama 3.2 3B Uncensored Q8_0 offers higher precision than its Q4 counterpart while maintaining uncensored, instruction-tuned capabilities. (Requires ~10GB RAM)",
  "info_url": "https://huggingface.co/nidum/Nidum-Llama-3.2-3B-Uncensored-GGUF"
},{
    "name": "DeepSeek-R1 Distill Qwen 1.5B",
    "size": "2.00 GB",
    "id": "deepseek_r1_distill_qwen_1.5b",
    "file_name": "DeepSeek-R1-Distill-Qwen-1.5B-Q8_0.gguf",
    "hf_link": "hf:unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/DeepSeek-R1-Distill-Qwen-1.5B-Q8_0.gguf",
    "category": "Specialized - Small",
    "description": "DeepSeek’s new R1 model sets new benchmarks in reasoning performance, matching OpenAI’s o1 model. (Needs ~8GB RAM)",
    "info_url": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF"
  },{
  "name": "SmolLM2 1.7B Instruct",
  "size": "2.00 GB",
  "id": "smollm2_1.7b_instruct",
  "file_name": "smollm2-1.7b-instruct-q4_k_m.gguf",
  "hf_link": "hf:HuggingFaceTB/SmolLM2-1.7B-Instruct-GGUF/smollm2-1.7b-instruct-q4_k_m.gguf",
  "category": "General Purpose - Small",
  "description": "SmolLM2 is a family of compact language models. They are capable of solving a wide range of tasks while being lightweight enough to run on-device. More details in our paper: https://arxiv.org/abs/2502.02737v1 (Needs ~8GB RAM)",
  "info_url": "https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct-GGUF"
},{
    "name": "Llama 3.2 1B",
    "size": "1.98 GB",
    "id": "llama3.2-1b",
    "file_name": "llama3.2_1b_q8.gguf",
    "hf_link": "hf:mradermacher/Llama-3.2-1B-Instruct-GGUF/Llama-3.2-1B-Instruct.Q8_0.gguf",
    "category": "General Purpose - Small",
    "description": "Llama 3.2 1B Q8 is a small and instruction-tuned model from Meta with higher precision quantization (Needs 8GB RAM)",
    "info_url": "https://l1nx.ai/YiAKr"
  },{
    "name": "Llama 3.1 Nemotron 70B Instruct",
    "size": "40.00 GB",
    "id": "llama-3.1-nemotron-70b",
    "file_name": "Llama-3.1-Nemotron-70B-Instruct-HF.i1-Q4_K_M.gguf",
    "hf_link": "hf:mradermacher/Llama-3.1-Nemotron-70B-Instruct-HF-i1-GGUF/Llama-3.1-Nemotron-70B-Instruct-HF.i1-Q4_K_M.gguf",
    "category": "Specialized - Large",
    "description": "NVIDIA's Llama 3.1 Nemotron 70B is a powerful model with 4-bit quantization (Q4_0). (Needs 40GB RAM)",
    "info_url": "https://l1nx.ai/zBGYb"
  },{
    "name": "DeepSeek-R1 Distill Llama 8B",
    "size": "17.00 GB",
    "id": "deepseek_r1_distill_llama_8b",
    "file_name": "DeepSeek-R1-Distill-Llama-8B-Q4_K_M.gguf",
    "hf_link": "hf:unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/DeepSeek-R1-Distill-Llama-8B-Q4_K_M.gguf",
    "category": "Specialized - Large",
    "description": "DeepSeek-R1 Distill Llama 8B is tuned using RL and excels in reasoning tasks. (Needs 16GB RAM)",
    "info_url": "https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF"
  },
  {
  "name": "Llama 3.1 Nemotron 8B UltraLong 4M Instruct",
  "size": "4.93 GB",
  "id": "llama-3.1-nemotron-8b-ultralong",
  "file_name": "llama-3.1-nemotron-8b-ultralong-4m-instruct-q4_k_m.gguf",
  "hf_link": "hf:itlwas/Llama-3.1-Nemotron-8B-UltraLong-4M-Instruct-Q4_K_M-GGUF/llama-3.1-nemotron-8b-ultralong-4m-instruct-q4_k_m.gguf",
  "category": "Specialized - Medium",
  "description": "Llama 3.1 Nemotron 8B UltraLong 4M is a special variant optimized for ultra-long context windows up to 4 million tokens, ideal for lengthy document reasoning and summarization. (Requires ~12GB RAM)",
  "info_url": "https://huggingface.co/itlwas/Llama-3.1-Nemotron-8B-UltraLong-4M-Instruct-Q4_K_M-GGUF"
}

]
